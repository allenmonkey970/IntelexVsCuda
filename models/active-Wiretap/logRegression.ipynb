{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearnex import patch_sklearn\n",
    "import cupy as cp \n",
    "import warnings"
   ],
   "id": "6ee78257a7bb2fd6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-08T05:30:34.681681Z",
     "start_time": "2025-03-08T05:28:26.688106Z"
    }
   },
   "source": [
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "def mainBareBones():\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        df = pd.read_csv('Active_Wiretap_dataset.csv', header=None, low_memory=False)\n",
    "        labels = pd.read_csv('Active_Wiretap_labels.csv', names=['label'], header=None, low_memory=False)\n",
    "\n",
    "        min_rows = min(len(df), len(labels))\n",
    "        df = df.iloc[:min_rows]\n",
    "        labels = labels.iloc[:min_rows]\n",
    "        labels['label'] = pd.to_numeric(labels['label'], errors='coerce')\n",
    "\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Label values found: {labels['label'].unique()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        exit()\n",
    "\n",
    "    # Preprocess and clean data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    df.columns = [f\"feature_{i}\" for i in range(df.shape[1])]\n",
    "    df['label'] = labels['label']\n",
    "    df = optimize_dtypes(df)\n",
    "    df = df.dropna()\n",
    "    df = df[df['label'].notna()]\n",
    "    print(f\"Shape after cleaning: {df.shape}\")\n",
    "\n",
    "\n",
    "    # Split features and labels\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Feature scaling\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Feature selection\n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(30, X.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    # Train Logistic Regression model\n",
    "    print(\"\\nTraining the Logistic Regression model...\")\n",
    "    model = LogisticRegression(\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Predictions and evaluation\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nProcess completed!\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
    "    print(f\"Label distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainBareBones()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (2278689, 115)\n",
      "Labels shape: (2278689, 1)\n",
      "Label values found: [nan  0.  1.]\n",
      "\n",
      "Preprocessing data...\n",
      "Shape after cleaning: (2278688, 116)\n",
      "Scaling features...\n",
      "Performing feature selection...\n",
      "\n",
      "Training the Logistic Regression model...\n",
      "Making predictions...\n",
      "\n",
      "Model Evaluation:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96    406644\n",
      "         1.0       0.92      0.95      0.94    276963\n",
      "\n",
      "    accuracy                           0.95    683607\n",
      "   macro avg       0.95      0.95      0.95    683607\n",
      "weighted avg       0.95      0.95      0.95    683607\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[385293  21351]\n",
      " [ 13689 263274]]\n",
      "\n",
      "Process completed!\n",
      "Total samples: 2278688\n",
      "Number of features: 115\n",
      "Number of selected features: 30\n",
      "Label distribution:\n",
      "label\n",
      "0.0    1355473\n",
      "1.0     923215\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T05:32:32.812071Z",
     "start_time": "2025-03-08T05:30:35.144200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import cupy as cp  # Import CuPy for GPU-based computation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "def mainWithCuPy():\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        df = pd.read_csv('Active_Wiretap_dataset.csv', header=None, low_memory=False)\n",
    "        labels = pd.read_csv('Active_Wiretap_labels.csv', names=['label'], header=None, low_memory=False)\n",
    "\n",
    "        min_rows = min(len(df), len(labels))\n",
    "        df = df.iloc[:min_rows]\n",
    "        labels = labels.iloc[:min_rows]\n",
    "        labels['label'] = pd.to_numeric(labels['label'], errors='coerce')\n",
    "\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Label values found: {labels['label'].unique()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        exit()\n",
    "\n",
    "    # Preprocess and clean data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    df.columns = [f\"feature_{i}\" for i in range(df.shape[1])]\n",
    "    df['label'] = labels['label']\n",
    "    df = optimize_dtypes(df)\n",
    "    df = df.dropna()\n",
    "    df = df[df['label'].notna()]\n",
    "    print(f\"Shape after cleaning: {df.shape}\")\n",
    "\n",
    "\n",
    "    # Transfer data to GPU using CuPy\n",
    "    print(\"\\nTransferring data to GPU...\")\n",
    "    X = cp.array(df.drop('label', axis=1).values)\n",
    "    y = cp.array(df['label'].values)\n",
    "\n",
    "    # Split features and labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Feature scaling\n",
    "    print(\"Scaling features on GPU...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(cp.asnumpy(X_train))  # Convert to NumPy for compatibility\n",
    "    X_test = scaler.transform(cp.asnumpy(X_test))\n",
    "    X_train = cp.array(X_train)  # Convert back to CuPy\n",
    "    X_test = cp.array(X_test)\n",
    "\n",
    "    # Feature selection\n",
    "    print(\"Performing feature selection on GPU...\")\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(30, X.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(cp.asnumpy(X_train), cp.asnumpy(y_train))\n",
    "    X_test_selected = selector.transform(cp.asnumpy(X_test))\n",
    "    X_train_selected = cp.array(X_train_selected)\n",
    "    X_test_selected = cp.array(X_test_selected)\n",
    "\n",
    "    # Train Logistic Regression model\n",
    "    print(\"\\nTraining the Logistic Regression model\")\n",
    "    model = LogisticRegression(\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(cp.asnumpy(X_train_selected), cp.asnumpy(y_train))  # Convert to NumPy for training\n",
    "\n",
    "    # Predictions and evaluation\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred = model.predict(cp.asnumpy(X_test_selected))\n",
    "\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(cp.asnumpy(y_test), y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(cp.asnumpy(y_test), y_pred))\n",
    "\n",
    "    print(\"\\nProcess completed!\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
    "    print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainWithCuPy()\n"
   ],
   "id": "c087fde7fb8fc368",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (2278689, 115)\n",
      "Labels shape: (2278689, 1)\n",
      "Label values found: [nan  0.  1.]\n",
      "\n",
      "Preprocessing data...\n",
      "Shape after cleaning: (2278688, 116)\n",
      "\n",
      "Transferring data to GPU...\n",
      "Scaling features on GPU...\n",
      "Performing feature selection on GPU...\n",
      "\n",
      "Training the Logistic Regression model\n",
      "Making predictions...\n",
      "\n",
      "Model Evaluation:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96    406644\n",
      "         1.0       0.93      0.95      0.94    276963\n",
      "\n",
      "    accuracy                           0.95    683607\n",
      "   macro avg       0.95      0.95      0.95    683607\n",
      "weighted avg       0.95      0.95      0.95    683607\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[385365  21279]\n",
      " [ 13750 263213]]\n",
      "\n",
      "Process completed!\n",
      "Total samples: 2278688\n",
      "Number of features: 115\n",
      "Number of selected features: 30\n",
      "Label distribution:\n",
      "label\n",
      "0.0    1355473\n",
      "1.0     923215\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "patch_sklearn() # Apply Intel optimizations",
   "id": "80358e2fe0d9a02c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-10T04:22:09.876512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "def mainIntelex():\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        df = pd.read_csv('Active_Wiretap_dataset.csv', header=None, low_memory=False)\n",
    "        labels = pd.read_csv('Active_Wiretap_labels.csv', names=['label'], header=None, low_memory=False)\n",
    "\n",
    "        min_rows = min(len(df), len(labels))\n",
    "        df = df.iloc[:min_rows]\n",
    "        labels = labels.iloc[:min_rows]\n",
    "        labels['label'] = pd.to_numeric(labels['label'], errors='coerce')\n",
    "\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Label values found: {labels['label'].unique()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        exit()\n",
    "\n",
    "    # Preprocess and clean data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    df.columns = [f\"feature_{i}\" for i in range(df.shape[1])]\n",
    "    df['label'] = labels['label']\n",
    "    df = optimize_dtypes(df)\n",
    "    df = df.dropna()\n",
    "    df = df[df['label'].notna()]\n",
    "    print(f\"Shape after cleaning: {df.shape}\")\n",
    "    \n",
    "    \n",
    "    # Split features and labels\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Feature scaling\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Feature selection\n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(30, X.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    # Train Logistic Regression model\n",
    "    print(\"\\nTraining the Logistic Regression model...\")\n",
    "    model = LogisticRegression(\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Predictions and evaluation\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nProcess completed!\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
    "    print(f\"Label distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainIntelex()\n"
   ],
   "id": "125b4c1e85fa478a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 93\u001B[0m\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLabel distribution:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 93\u001B[0m     mainIntelex()\n",
      "Cell \u001B[1;32mIn[1], line 25\u001B[0m, in \u001B[0;36mmainIntelex\u001B[1;34m()\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading dataset...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 25\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActive_Wiretap_dataset.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, low_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     26\u001B[0m     labels \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActive_Wiretap_labels.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m], header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, low_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     28\u001B[0m     min_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mlen\u001B[39m(df), \u001B[38;5;28mlen\u001B[39m(labels))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\u001B[38;5;241m.\u001B[39mread(nrows)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1919\u001B[0m     (\n\u001B[0;32m   1920\u001B[0m         index,\n\u001B[0;32m   1921\u001B[0m         columns,\n\u001B[0;32m   1922\u001B[0m         col_dict,\n\u001B[1;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mread(  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m         nrows\n\u001B[0;32m   1925\u001B[0m     )\n\u001B[0;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 239\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader\u001B[38;5;241m.\u001B[39mread(nrows)\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_first_chunk:\n",
      "File \u001B[1;32mparsers.pyx:820\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:914\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:891\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:2053\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\sklearn-env\\Lib\\codecs.py:322\u001B[0m, in \u001B[0;36mBufferedIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_buffer_decode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, errors, final):\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001B[39;00m\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;66;03m# and return an (output, length consumed) tuple\u001B[39;00m\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n\u001B[1;32m--> 322\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;66;03m# decode input (taking the buffer into account)\u001B[39;00m\n\u001B[0;32m    324\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m+\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[0;32m    325\u001B[0m     (result, consumed) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer_decode(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors, final)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T05:34:30.923074Z",
     "start_time": "2025-03-08T05:34:30.920763Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "14bb4ffdb6db59fd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
