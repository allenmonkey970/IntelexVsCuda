{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-15T18:36:12.552662Z",
     "start_time": "2025-03-15T18:36:10.008420Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "import cupy as cp\n",
    "from sklearnex import patch_sklearn\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T18:44:54.077646Z",
     "start_time": "2025-03-15T18:36:12.556543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "def mainBareBones():\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        # Load main dataset\n",
    "        df = pd.read_csv('Active_Wiretap_dataset.csv', \n",
    "                         header=None,\n",
    "                         low_memory=False)\n",
    "        \n",
    "        # Load labels and handle mixed types\n",
    "        labels = pd.read_csv('Active_Wiretap_labels.csv', \n",
    "                            names=['label'], \n",
    "                            header=None,\n",
    "                            low_memory=False)\n",
    "        \n",
    "        # Ensure datasets have the same number of rows\n",
    "        min_rows = min(len(df), len(labels))\n",
    "        df = df.iloc[:min_rows]\n",
    "        labels = labels.iloc[:min_rows]\n",
    "        \n",
    "        # Convert labels to numeric\n",
    "        labels['label'] = pd.to_numeric(labels['label'], errors='coerce')\n",
    "        \n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Label values found: {labels['label'].unique()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Preprocess the data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    n_features = df.shape[1]\n",
    "    column_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "    df.columns = column_names\n",
    "    df['label'] = labels['label']\n",
    "\n",
    "    # Step 3: Optimize memory usage\n",
    "    print(\"Optimizing memory usage...\")\n",
    "    df = optimize_dtypes(df)\n",
    "\n",
    "    # Step 4: Clean data\n",
    "    print(\"Cleaning data...\")\n",
    "    # Remove any rows with missing values\n",
    "    df = df.dropna()\n",
    "    # Remove any rows where the label is NaN\n",
    "    df = df[df['label'].notna()]\n",
    "\n",
    "    print(f\"Shape after cleaning: {df.shape}\")\n",
    "\n",
    "    # Step 5: Split features and labels\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "\n",
    "    # Step 6: Split the data\n",
    "    print(\"\\nSplitting data into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Step 7: Feature selection\n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(30, X.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    # Step 8: Train model\n",
    "    print(\"\\nTraining the MLP model...\")\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(50, 30),  \n",
    "        max_iter=1200,               \n",
    "        learning_rate='adaptive',            \n",
    "        early_stopping=True,        \n",
    "        random_state=42              \n",
    "    )\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    print(\"Model training completed!\")\n",
    "\n",
    "    # Step 9: Predictions\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "\n",
    "    # Step 10: Evaluation\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Additional information\n",
    "    print(\"\\nProcess completed!\")\n",
    "\n",
    "    print(\"\\nDataset Information:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
    "    print(f\"Label distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainBareBones()"
   ],
   "id": "63094d3ecc2124cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (2278689, 115)\n",
      "Labels shape: (2278689, 1)\n",
      "Label values found: [nan  0.  1.]\n",
      "\n",
      "Preprocessing data...\n",
      "Optimizing memory usage...\n",
      "Cleaning data...\n",
      "Shape after cleaning: (2278688, 116)\n",
      "\n",
      "Splitting data into train and test sets...\n",
      "Performing feature selection...\n",
      "\n",
      "Training the MLP model...\n",
      "Model training completed!\n",
      "Making predictions...\n",
      "\n",
      "Model Evaluation:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.96      0.97    406644\n",
      "         1.0       0.94      0.98      0.96    276963\n",
      "\n",
      "    accuracy                           0.97    683607\n",
      "   macro avg       0.96      0.97      0.97    683607\n",
      "weighted avg       0.97      0.97      0.97    683607\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[389456  17188]\n",
      " [  4378 272585]]\n",
      "\n",
      "Process completed!\n",
      "\n",
      "Dataset Information:\n",
      "Total samples: 2278688\n",
      "Number of features: 115\n",
      "Number of selected features: 30\n",
      "Label distribution:\n",
      "label\n",
      "0.0    1355473\n",
      "1.0     923215\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T18:49:39.159759Z",
     "start_time": "2025-03-15T18:44:54.284490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = cp.array(df[col], dtype=cp.float32).get()\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = cp.array(df[col], dtype=cp.int32).get()\n",
    "    return df\n",
    "\n",
    "def mainCuPY():\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        # Load main dataset\n",
    "        df = pd.read_csv('Active_Wiretap_dataset.csv', \n",
    "                         header=None,\n",
    "                         low_memory=False)\n",
    "        \n",
    "        # Load labels and handle mixed types\n",
    "        labels = pd.read_csv('Active_Wiretap_labels.csv', \n",
    "                            names=['label'], \n",
    "                            header=None,\n",
    "                            low_memory=False)\n",
    "        \n",
    "        # Ensure datasets have the same number of rows\n",
    "        min_rows = min(len(df), len(labels))\n",
    "        df = df.iloc[:min_rows]\n",
    "        labels = labels.iloc[:min_rows]\n",
    "        \n",
    "        # Convert labels to numeric\n",
    "        labels['label'] = pd.to_numeric(labels['label'], errors='coerce')\n",
    "        \n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Label values found: {labels['label'].unique()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Preprocess the data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    n_features = df.shape[1]\n",
    "    column_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "    df.columns = column_names\n",
    "    df['label'] = labels['label']\n",
    "\n",
    "    # Optimize memory usage\n",
    "    print(\"Optimizing memory usage...\")\n",
    "    df = optimize_dtypes(df)\n",
    "\n",
    "    # Clean data\n",
    "    print(\"Cleaning data...\")\n",
    "    df = df.dropna()  # Drop rows with missing values\n",
    "    df = df[df['label'].notna()]  # Drop rows where label is NaN\n",
    "\n",
    "    print(f\"Shape after cleaning: {df.shape}\")\n",
    "\n",
    "    # Split features and labels\n",
    "    X = cp.array(df.drop('label', axis=1).values, dtype=cp.float32)\n",
    "    y = cp.array(df['label'].values, dtype=cp.int32) \n",
    "\n",
    "    # Split the data\n",
    "    print(\"\\nSplitting data into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.get(), y.get(), test_size=0.3, random_state=42)\n",
    "    print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "    # Feature selection\n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(30, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\nTraining the MLP model...\")\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(50, 30),  \n",
    "        max_iter=1200,               \n",
    "        learning_rate='adaptive',            \n",
    "        early_stopping=True,        \n",
    "        random_state=42              \n",
    "    )\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    print(\"Model training completed!\")\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "\n",
    "    # Model evaluation\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Additional information\n",
    "    print(\"\\nProcess completed!\")\n",
    "    print(\"\\nDataset Information:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
    "    print(f\"Label distribution:\\n{pd.Series(y.get()).value_counts()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainCuPY()\n"
   ],
   "id": "f417de1209a3e470",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (2278689, 115)\n",
      "Labels shape: (2278689, 1)\n",
      "Label values found: [nan  0.  1.]\n",
      "\n",
      "Preprocessing data...\n",
      "Optimizing memory usage...\n",
      "Cleaning data...\n",
      "Shape after cleaning: (2278688, 116)\n",
      "\n",
      "Splitting data into train and test sets...\n",
      "Training set shape: (1595081, 115), Testing set shape: (683607, 115)\n",
      "Performing feature selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:97: RuntimeWarning: overflow encountered in square\n",
      "  square_of_sums_alldata = sum(sums_args) ** 2\n",
      "C:\\Users\\antho\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:98: RuntimeWarning: overflow encountered in square\n",
      "  square_of_sums_args = [s**2 for s in sums_args]\n",
      "C:\\Users\\antho\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:103: RuntimeWarning: invalid value encountered in subtract\n",
      "  ssbn -= square_of_sums_alldata / float(n_samples)\n",
      "C:\\Users\\antho\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:104: RuntimeWarning: invalid value encountered in subtract\n",
      "  sswn = sstot - ssbn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the MLP model...\n",
      "Model training completed!\n",
      "Making predictions...\n",
      "\n",
      "Model Evaluation:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97    406644\n",
      "           1       0.95      0.97      0.96    276963\n",
      "\n",
      "    accuracy                           0.97    683607\n",
      "   macro avg       0.96      0.97      0.96    683607\n",
      "weighted avg       0.97      0.97      0.97    683607\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[392342  14302]\n",
      " [  9535 267428]]\n",
      "\n",
      "Process completed!\n",
      "\n",
      "Dataset Information:\n",
      "Total samples: 2278688\n",
      "Number of features: 115\n",
      "Number of selected features: 30\n",
      "Label distribution:\n",
      "0    1355473\n",
      "1     923215\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T18:49:39.260623Z",
     "start_time": "2025-03-15T18:49:39.167353Z"
    }
   },
   "cell_type": "code",
   "source": "patch_sklearn() # Apply Intel optimizations",
   "id": "7a9e15bd4f28241",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T18:58:21.489625Z",
     "start_time": "2025-03-15T18:49:39.269928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "def mainIntelex():\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        # Load main dataset\n",
    "        df = pd.read_csv('Active_Wiretap_dataset.csv', \n",
    "                         header=None,\n",
    "                         low_memory=False)\n",
    "        \n",
    "        # Load labels and handle mixed types\n",
    "        labels = pd.read_csv('Active_Wiretap_labels.csv', \n",
    "                            names=['label'], \n",
    "                            header=None,\n",
    "                            low_memory=False)\n",
    "        \n",
    "        # Ensure datasets have the same number of rows\n",
    "        min_rows = min(len(df), len(labels))\n",
    "        df = df.iloc[:min_rows]\n",
    "        labels = labels.iloc[:min_rows]\n",
    "        \n",
    "        # Convert labels to numeric\n",
    "        labels['label'] = pd.to_numeric(labels['label'], errors='coerce')\n",
    "        \n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Label values found: {labels['label'].unique()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Preprocess the data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    n_features = df.shape[1]\n",
    "    column_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "    df.columns = column_names\n",
    "    df['label'] = labels['label']\n",
    "\n",
    "    # Step 3: Optimize memory usage\n",
    "    print(\"Optimizing memory usage...\")\n",
    "    df = optimize_dtypes(df)\n",
    "\n",
    "    # Step 4: Clean data\n",
    "    print(\"Cleaning data...\")\n",
    "    # Remove any rows with missing values\n",
    "    df = df.dropna()\n",
    "    # Remove any rows where the label is NaN\n",
    "    df = df[df['label'].notna()]\n",
    "\n",
    "    print(f\"Shape after cleaning: {df.shape}\")\n",
    "\n",
    "    # Step 5: Split features and labels\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "\n",
    "    # Step 6: Split the data\n",
    "    print(\"\\nSplitting data into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Step 7: Feature selection\n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(30, X.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    # Step 8: Train model\n",
    "    print(\"\\nTraining the MLP model...\")\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(50, 30),  \n",
    "        max_iter=1200,               \n",
    "        learning_rate='adaptive',            \n",
    "        early_stopping=True,        \n",
    "        random_state=42              \n",
    "    )\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    print(\"Model training completed!\")\n",
    "\n",
    "    # Step 9: Predictions\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "\n",
    "    # Step 10: Evaluation\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Additional information\n",
    "    print(\"\\nProcess completed!\")\n",
    "\n",
    "    print(\"\\nDataset Information:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
    "    print(f\"Label distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainIntelex()"
   ],
   "id": "9e51e4f15eedbebc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (2278689, 115)\n",
      "Labels shape: (2278689, 1)\n",
      "Label values found: [nan  0.  1.]\n",
      "\n",
      "Preprocessing data...\n",
      "Optimizing memory usage...\n",
      "Cleaning data...\n",
      "Shape after cleaning: (2278688, 116)\n",
      "\n",
      "Splitting data into train and test sets...\n",
      "Performing feature selection...\n",
      "\n",
      "Training the MLP model...\n",
      "Model training completed!\n",
      "Making predictions...\n",
      "\n",
      "Model Evaluation:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.96      0.97    406644\n",
      "         1.0       0.94      0.98      0.96    276963\n",
      "\n",
      "    accuracy                           0.97    683607\n",
      "   macro avg       0.96      0.97      0.97    683607\n",
      "weighted avg       0.97      0.97      0.97    683607\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[389456  17188]\n",
      " [  4378 272585]]\n",
      "\n",
      "Process completed!\n",
      "\n",
      "Dataset Information:\n",
      "Total samples: 2278688\n",
      "Number of features: 115\n",
      "Number of selected features: 30\n",
      "Label distribution:\n",
      "label\n",
      "0.0    1355473\n",
      "1.0     923215\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
