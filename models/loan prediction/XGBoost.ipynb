{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T00:06:48.093659Z",
     "start_time": "2025-03-15T00:06:45.793919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import cupy as cp\n",
    "from sklearnex import patch_sklearn"
   ],
   "id": "c24f871718fd19ca",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-15T00:06:52.533399Z",
     "start_time": "2025-03-15T00:06:48.098441Z"
    }
   },
   "source": [
    "def optimize_datatypes(data):\n",
    "    # Downcast numerical columns to reduce memory usage\n",
    "    for col in data.select_dtypes(include=['int']):\n",
    "        data[col] = pd.to_numeric(data[col], downcast='integer')\n",
    "    for col in data.select_dtypes(include=['float']):\n",
    "        data[col] = pd.to_numeric(data[col], downcast='float')\n",
    "    return data\n",
    "\n",
    "def mainBareBones():\n",
    "    # Load the dataset\n",
    "    file_path = \"Loan Prediction.csv\"  # Replace with your dataset's path\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully! Shape: {data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Data Cleaning\n",
    "    print(\"\\nCleaning and preprocessing data...\")\n",
    "    missing_values_before = data.isnull().sum().sum()\n",
    "    print(f\"Missing values before cleaning: {missing_values_before}\")\n",
    "    \n",
    "    # Fill missing values for categorical and numerical columns\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'object':  # Categorical columns\n",
    "            data[col] = data[col].fillna(data[col].mode()[0])\n",
    "        else:  # Numerical columns\n",
    "            data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    missing_values_after = data.isnull().sum().sum()\n",
    "    print(f\"Missing values after cleaning: {missing_values_after}\")\n",
    "\n",
    "    # Optimize datatypes to reduce memory usage\n",
    "    print(\"\\nOptimizing datatypes...\")\n",
    "    data = optimize_datatypes(data)\n",
    "\n",
    "    # Convert categorical variables to numeric with one-hot encoding\n",
    "    print(\"Converting categorical variables to numeric with one-hot encoding...\")\n",
    "    categorical_columns = ['Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE']\n",
    "    data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "    print(f\"Data shape after encoding: {data.shape}\")\n",
    "\n",
    "    # Feature-target split\n",
    "    print(\"\\nSeparating features and target variable...\")\n",
    "    X = data.drop(columns=['Risk_Flag', 'Id'])  # Dropping Id and target column\n",
    "    y = data['Risk_Flag']\n",
    "    print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "\n",
    "    # Sanitize column names for compatibility with XGBoost\n",
    "    print(\"\\nSanitizing column names for XGBoost compatibility...\")\n",
    "    X.columns = [col.replace('[', '').replace(']', '').replace('<', '').replace('>', '').replace(' ', '_') for col in X.columns]\n",
    "\n",
    "    # Convert data to optimized types\n",
    "    print(\"\\nConverting data to optimized types...\")\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int32)\n",
    "\n",
    "    # Train-test split\n",
    "    print(\"\\nSplitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "    # Convert to DMatrix\n",
    "    print(\"\\nConverting data to DMatrix format for XGBoost...\")\n",
    "    train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "    test_dmatrix = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Parameters for XGBoost\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'hist',  # Optimized for GPU\n",
    "        'device': 'cuda:0', # Use GPU,\n",
    "        'random_state': 42 \n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    print(\"\\nTraining the XGBoost model...\")\n",
    "    model = xgb.train(params, train_dmatrix, num_boost_round=100)\n",
    "    print(\"Model training completed!\")\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions on the test set...\")\n",
    "    y_pred = model.predict(test_dmatrix)\n",
    "    y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating the model...\")\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainBareBones()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully! Shape: (252000, 13)\n",
      "\n",
      "Cleaning and preprocessing data...\n",
      "Missing values before cleaning: 0\n",
      "Missing values after cleaning: 0\n",
      "\n",
      "Optimizing datatypes...\n",
      "Converting categorical variables to numeric with one-hot encoding...\n",
      "Data shape after encoding: (252000, 405)\n",
      "\n",
      "Separating features and target variable...\n",
      "Features shape: (252000, 403), Target shape: (252000,)\n",
      "\n",
      "Sanitizing column names for XGBoost compatibility...\n",
      "\n",
      "Converting data to optimized types...\n",
      "\n",
      "Splitting data into training and testing sets...\n",
      "Training set shape: (201600, 403), Testing set shape: (50400, 403)\n",
      "\n",
      "Converting data to DMatrix format for XGBoost...\n",
      "\n",
      "Training the XGBoost model...\n",
      "Model training completed!\n",
      "\n",
      "Making predictions on the test set...\n",
      "\n",
      "Evaluating the model...\n",
      "Accuracy: 88.06%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94     44147\n",
      "           1       0.68      0.07      0.13      6253\n",
      "\n",
      "    accuracy                           0.88     50400\n",
      "   macro avg       0.78      0.53      0.53     50400\n",
      "weighted avg       0.86      0.88      0.84     50400\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T00:06:56.314814Z",
     "start_time": "2025-03-15T00:06:52.704073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_datatypes(data):\n",
    "    # Downcast numerical columns to reduce memory usage\n",
    "    for col in data.select_dtypes(include=['int']):\n",
    "        data[col] = pd.to_numeric(data[col], downcast='integer')\n",
    "    for col in data.select_dtypes(include=['float']):\n",
    "        data[col] = pd.to_numeric(data[col], downcast='float')\n",
    "    return data\n",
    "\n",
    "def mainCuPY():\n",
    "    # Load the dataset\n",
    "    file_path = \"Loan Prediction.csv\"  # Replace with the path to your dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully! Shape: {data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Data Cleaning\n",
    "    print(\"\\nCleaning and preprocessing data...\")\n",
    "    missing_values_before = data.isnull().sum().sum()\n",
    "    print(f\"Missing values before cleaning: {missing_values_before}\")\n",
    "\n",
    "    # Fill missing values for categorical and numerical columns\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'object':  # Categorical columns\n",
    "            data[col] = data[col].fillna(data[col].mode()[0])\n",
    "        else:  # Numerical columns\n",
    "            data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    missing_values_after = data.isnull().sum().sum()\n",
    "    print(f\"Missing values after cleaning: {missing_values_after}\")\n",
    "\n",
    "    # Optimize datatypes to reduce memory usage\n",
    "    print(\"\\nOptimizing datatypes...\")\n",
    "    data = optimize_datatypes(data)\n",
    "\n",
    "    # Convert categorical variables to numeric\n",
    "    print(\"Converting categorical variables to numeric with one-hot encoding...\")\n",
    "    categorical_columns = ['Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE']\n",
    "    data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "    print(f\"Data shape after encoding: {data.shape}\")\n",
    "\n",
    "    # Feature-target split\n",
    "    print(\"\\nSeparating features and target variable...\")\n",
    "    X = data.drop(columns=['Risk_Flag', 'Id'])  # Dropping Id and target column\n",
    "    y = data['Risk_Flag']\n",
    "    print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "\n",
    "    # Sanitize column names for compatibility with XGBoost\n",
    "    print(\"\\nSanitizing column names for XGBoost compatibility...\")\n",
    "    X.columns = [col.replace('[', '').replace(']', '').replace('<', '').replace('>', '').replace(' ', '_') for col in X.columns]\n",
    "\n",
    "    # Convert features and target to optimized numeric types\n",
    "    print(\"\\nConverting data to optimized types...\")\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int32)\n",
    "\n",
    "    # Train-test split\n",
    "    print(\"\\nSplitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "    # Convert data to CuPy arrays\n",
    "    print(\"\\nConverting data to CuPy arrays...\")\n",
    "    X_train_cp = cp.asarray(X_train)\n",
    "    X_test_cp = cp.asarray(X_test)\n",
    "    y_train_cp = cp.asarray(y_train)\n",
    "    y_test_cp = cp.asarray(y_test)\n",
    "\n",
    "    # Convert CuPy arrays to DMatrix for XGBoost\n",
    "    print(\"\\nConverting data to DMatrix format for XGBoost...\")\n",
    "    train_dmatrix = xgb.DMatrix(X_train_cp, label=cp.asnumpy(y_train_cp))\n",
    "    test_dmatrix = xgb.DMatrix(X_test_cp, label=cp.asnumpy(y_test_cp))\n",
    "\n",
    "    # Parameters for XGBoost\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'hist',  # Optimized for GPU\n",
    "        'device': 'cuda:0', # Use GPU,\n",
    "        'random_state': 42 \n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    print(\"\\nTraining the XGBoost model...\")\n",
    "    model = xgb.train(params, train_dmatrix, num_boost_round=100)\n",
    "    print(\"Model training completed!\")\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions on the test set...\")\n",
    "    y_pred = model.predict(test_dmatrix)\n",
    "    y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating the model...\")\n",
    "    accuracy = accuracy_score(cp.asnumpy(y_test_cp), y_pred_binary)\n",
    "    print(f\"Accuracy (CuPy Optimized): {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(cp.asnumpy(y_test_cp), y_pred_binary))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainCuPY()"
   ],
   "id": "c7e9bbeb2ad28df9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully! Shape: (252000, 13)\n",
      "\n",
      "Cleaning and preprocessing data...\n",
      "Missing values before cleaning: 0\n",
      "Missing values after cleaning: 0\n",
      "\n",
      "Optimizing datatypes...\n",
      "Converting categorical variables to numeric with one-hot encoding...\n",
      "Data shape after encoding: (252000, 405)\n",
      "\n",
      "Separating features and target variable...\n",
      "Features shape: (252000, 403), Target shape: (252000,)\n",
      "\n",
      "Sanitizing column names for XGBoost compatibility...\n",
      "\n",
      "Converting data to optimized types...\n",
      "\n",
      "Splitting data into training and testing sets...\n",
      "Training set shape: (201600, 403), Testing set shape: (50400, 403)\n",
      "\n",
      "Converting data to CuPy arrays...\n",
      "\n",
      "Converting data to DMatrix format for XGBoost...\n",
      "\n",
      "Training the XGBoost model...\n",
      "Model training completed!\n",
      "\n",
      "Making predictions on the test set...\n",
      "\n",
      "Evaluating the model...\n",
      "Accuracy (CuPy Optimized): 88.06%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94     44147\n",
      "           1       0.68      0.07      0.13      6253\n",
      "\n",
      "    accuracy                           0.88     50400\n",
      "   macro avg       0.78      0.53      0.53     50400\n",
      "weighted avg       0.86      0.88      0.84     50400\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "patch_sklearn() # Apply Intel optimizations",
   "id": "5b042dc7f1febfc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T00:07:00.492599Z",
     "start_time": "2025-03-15T00:06:56.321627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_datatypes(data):\n",
    "    # Downcast numerical columns to reduce memory usage\n",
    "    for col in data.select_dtypes(include=['int']):\n",
    "        data[col] = pd.to_numeric(data[col], downcast='integer')\n",
    "    for col in data.select_dtypes(include=['float']):\n",
    "        data[col] = pd.to_numeric(data[col], downcast='float')\n",
    "    return data\n",
    "\n",
    "def mainIntelex():\n",
    "    # Load the dataset\n",
    "    file_path = \"Loan Prediction.csv\"  # Replace with your dataset's path\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully! Shape: {data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Data Cleaning\n",
    "    print(\"\\nCleaning and preprocessing data...\")\n",
    "    missing_values_before = data.isnull().sum().sum()\n",
    "    print(f\"Missing values before cleaning: {missing_values_before}\")\n",
    "    \n",
    "    # Fill missing values for categorical and numerical columns\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'object':  # Categorical columns\n",
    "            data[col] = data[col].fillna(data[col].mode()[0])\n",
    "        else:  # Numerical columns\n",
    "            data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    missing_values_after = data.isnull().sum().sum()\n",
    "    print(f\"Missing values after cleaning: {missing_values_after}\")\n",
    "\n",
    "    # Optimize datatypes to reduce memory usage\n",
    "    print(\"\\nOptimizing datatypes...\")\n",
    "    data = optimize_datatypes(data)\n",
    "\n",
    "    # Convert categorical variables to numeric with one-hot encoding\n",
    "    print(\"Converting categorical variables to numeric with one-hot encoding...\")\n",
    "    categorical_columns = ['Married/Single', 'House_Ownership', 'Car_Ownership', 'Profession', 'CITY', 'STATE']\n",
    "    data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "    print(f\"Data shape after encoding: {data.shape}\")\n",
    "\n",
    "    # Feature-target split\n",
    "    print(\"\\nSeparating features and target variable...\")\n",
    "    X = data.drop(columns=['Risk_Flag', 'Id'])  # Dropping Id and target column\n",
    "    y = data['Risk_Flag']\n",
    "    print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "\n",
    "    # Sanitize column names for compatibility with XGBoost\n",
    "    print(\"\\nSanitizing column names for XGBoost compatibility...\")\n",
    "    X.columns = [col.replace('[', '').replace(']', '').replace('<', '').replace('>', '').replace(' ', '_') for col in X.columns]\n",
    "\n",
    "    # Convert data to optimized types\n",
    "    print(\"\\nConverting data to optimized types...\")\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int32)\n",
    "\n",
    "    # Train-test split\n",
    "    print(\"\\nSplitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "\n",
    "    # Convert to DMatrix\n",
    "    print(\"\\nConverting data to DMatrix format for XGBoost...\")\n",
    "    train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "    test_dmatrix = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Parameters for XGBoost\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'hist',  # Optimized for GPU\n",
    "        'device': 'cuda:0', # Use GPU,\n",
    "        'random_state': 42 \n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    print(\"\\nTraining the XGBoost model...\")\n",
    "    model = xgb.train(params, train_dmatrix, num_boost_round=100)\n",
    "    print(\"Model training completed!\")\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions on the test set...\")\n",
    "    y_pred = model.predict(test_dmatrix)\n",
    "    y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating the model...\")\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mainIntelex()"
   ],
   "id": "c957a9812123aade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully! Shape: (252000, 13)\n",
      "\n",
      "Cleaning and preprocessing data...\n",
      "Missing values before cleaning: 0\n",
      "Missing values after cleaning: 0\n",
      "\n",
      "Optimizing datatypes...\n",
      "Converting categorical variables to numeric with one-hot encoding...\n",
      "Data shape after encoding: (252000, 405)\n",
      "\n",
      "Separating features and target variable...\n",
      "Features shape: (252000, 403), Target shape: (252000,)\n",
      "\n",
      "Sanitizing column names for XGBoost compatibility...\n",
      "\n",
      "Converting data to optimized types...\n",
      "\n",
      "Splitting data into training and testing sets...\n",
      "Training set shape: (201600, 403), Testing set shape: (50400, 403)\n",
      "\n",
      "Converting data to DMatrix format for XGBoost...\n",
      "\n",
      "Training the XGBoost model...\n",
      "Model training completed!\n",
      "\n",
      "Making predictions on the test set...\n",
      "\n",
      "Evaluating the model...\n",
      "Accuracy: 88.06%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94     44147\n",
      "           1       0.68      0.07      0.13      6253\n",
      "\n",
      "    accuracy                           0.88     50400\n",
      "   macro avg       0.78      0.53      0.53     50400\n",
      "weighted avg       0.86      0.88      0.84     50400\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
